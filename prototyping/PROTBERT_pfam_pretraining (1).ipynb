{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDWOMAUVT_9r"
   },
   "source": [
    "# install dependecnies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MahFUTFSUCFq",
    "outputId": "bd9473c3-7ee0-4b8e-d351-2cc120a8cf69",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets\n",
    "!pip install -q transformers\n",
    "!pip install -q umap-learn\n",
    "!pip install -q bertviz\n",
    "!pip install -q accelerate\n",
    "!pip install -q seqeval\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn \n",
    "!pip install -q transformers\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3LR-uVbRF9-",
    "outputId": "2dd1a42e-e32a-4465-b1dc-aa8601a2014a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.0.9\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
      "done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f5326d0be6f34b7a809d0acd13c02aa8"
     ]
    },
    "id": "sxUVdX6HRF9_",
    "outputId": "3a7f6b7e-1b10-484d-b31e-6a591adfa804",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18892fb8f5d44e0c8e3ff8a14a6580c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k__0_zfTUFhO"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oRdkAUW4UGeK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv7zY2CUIEe"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oUlxWWcXUJNq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_id = '1w3IQMK3PmXH-Bq6Lt_P8wxGPvr5olrZT'\n",
    "destination = 'pfam.zip'\n",
    "hf_name = \"t5-small\"\n",
    "n_families_of_interest = 1000\n",
    "data_dirpath = \"pfam\"\n",
    "aminoacid_separate_by = \"\"\n",
    "max_length = 512\n",
    "tokenizer_folder = \"PROTNAME_tok\"\n",
    "save_folder = \"PROTNAME\"\n",
    "vocab_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFjUpTN8UNyS"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "40KG3kyHUOtM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_size(value):\n",
    "  if value >= 1024**3:\n",
    "    return f\"{round(value / 1024**3 , 3)} GB\"\n",
    "  elif value >= 1024**2:\n",
    "    return f\"{round(value / 1024**2 , 3)} MB\"\n",
    "  elif value >= 1024:\n",
    "    return f\"{round(value / 1024 , 3)} KB\"\n",
    "  return f\"{value} B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DAYQ7M_TUPMr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code taken from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "    params = { 'id' : id, 'confirm' : 1 }\n",
    "    response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    \n",
    "    pb = tqdm.tqdm(response.iter_content(CHUNK_SIZE))\n",
    "    b_total = 0\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for i,chunk in enumerate(pb):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                b_total += len(chunk)\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                  pb.set_description(f\"written : {format_size(b_total)}\")\n",
    "                i += 1\n",
    "    print(\"\\n\")\n",
    "    print(f\"saved all the data to {destination}. total size : {format_size(os.stat(destination).st_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n-nQBqwTA7L"
   },
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTAn0zO0S8qL",
    "outputId": "47d3ac58-6bca-4de3-a419-7072804b6c15",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "written : 468.781 MB: : 15780it [00:11, 1368.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saved all the data to pfam.zip. total size : 493.095 MB\n"
     ]
    }
   ],
   "source": [
    "download_file_from_google_drive(file_id, destination)\n",
    "with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "!mv random_split pfam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20iK8FX3UnTN"
   },
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEkQLYGDTw2a",
    "outputId": "7a244f76-2853-4cc6-d931-049c729cd02b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset partitions:  ['train', 'test', 'random_split', 'dev']\n",
      "Dataset partition \"test\" has 126171 sequences\n",
      "Dataset partition \"dev\" has 126171 sequences\n",
      "Dataset partition \"train\" has 1086741 sequences\n",
      "how many labels : 17929\n",
      "1000 classes is 40.400000000000006 portion of training data\n"
     ]
    }
   ],
   "source": [
    "def read_all_shards(partition='dev', data_dir = data_dirpath):\n",
    "    shards = []\n",
    "    for fn in os.listdir(os.path.join(data_dir, partition)):\n",
    "        with open(os.path.join(data_dir, partition, fn)) as f:\n",
    "            shards.append(pd.read_csv(f, index_col=None))\n",
    "    \n",
    "    return pd.concat(shards)\n",
    "\n",
    "def read_all_data_initial():\n",
    "  global train, test, dev, all_train_ds_size, all_test_ds_size, all_dev_ds_size\n",
    "\n",
    "  test = read_all_shards('test')\n",
    "  dev = read_all_shards('dev')\n",
    "  train = read_all_shards('train')\n",
    "\n",
    "  partitions = {'test': test, 'dev': dev, 'train': train}\n",
    "  for name, df in partitions.items():\n",
    "      print('Dataset partition \"%s\" has %d sequences' % (name, len(df)))\n",
    "\n",
    "  all_train_ds_size = len(train)\n",
    "  all_test_ds_size = len(test)\n",
    "  all_dev_ds_size = len(dev)\n",
    "\n",
    "  train.reset_index(inplace=True, drop=True)\n",
    "  dev.reset_index(inplace=True, drop=True)\n",
    "  test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "def get_cumulative(data):\n",
    "    counter = Counter(data['family_accession'])\n",
    "    print(f\"how many labels : {len(counter)}\")\n",
    "    \n",
    "    datasetSize = len(data)\n",
    "    xs = []\n",
    "    x_labels = []\n",
    "    ys = []\n",
    "\n",
    "    t = 0\n",
    "    cumulative = []\n",
    "\n",
    "    for i,(x, y) in  enumerate(counter.most_common()):\n",
    "        xs.append(i)\n",
    "        x_labels.append(x)\n",
    "        ys.append(y)\n",
    "        t += y / datasetSize\n",
    "        cumulative.append(t)\n",
    "    return cumulative\n",
    "\n",
    "\n",
    "# EXECUTION CODE\n",
    "print('Available dataset partitions: ', os.listdir(data_dirpath))\n",
    "read_all_data_initial()\n",
    "cumulative = get_cumulative(train)\n",
    "print(f\"{n_families_of_interest} classes is {100 * round( cumulative[n_families_of_interest-1],3)} portion of training data\")\n",
    "\n",
    "familiesOfInterest = train.family_accession.value_counts()[:n_families_of_interest]\n",
    "\n",
    "mask = train.family_accession.isin(familiesOfInterest.index.values)\n",
    "train = train.loc[mask,:]\n",
    "\n",
    "mask = dev.family_accession.isin(familiesOfInterest.index.values)\n",
    "dev = dev.loc[mask,:]\n",
    "\n",
    "mask = test.family_accession.isin(familiesOfInterest.index.values)\n",
    "test = test.loc[mask,:]\n",
    "\n",
    "\n",
    "################################################################################\n",
    "train_seq = train['sequence']\n",
    "dev_seq = dev['sequence']\n",
    "test_seq = test['sequence']\n",
    "\n",
    "################################################################################\n",
    "train_sentences = train_seq.apply(lambda seq: aminoacid_separate_by.join([aa for aa in seq]))\n",
    "validation_sentences = dev_seq.apply(lambda seq: aminoacid_separate_by.join([aa for aa in seq]))\n",
    "test_sentences = test_seq.apply(lambda seq: aminoacid_separate_by.join([aa for aa in seq]))\n",
    "\n",
    "################################################################################\n",
    "train_labels = train['family_accession'].apply(lambda x: x.split('.')[0])\n",
    "validation_labels = dev['family_accession'].apply(lambda x: x.split('.')[0])\n",
    "test_labels = test['family_accession'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41vNsN2SWUf3",
    "outputId": "366dcfde-d773-4182-c1c3-2f23bd1e4b2b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    PF00365\n",
       "Name: family_accession, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"RIGIMTSGGDAPGMNLAIRAVARKALSSGLEAYGINYGFAGLVAGDIHEFKAADLDDMVSQGGTMLYSARYPEFAQEESQLKGIEQLKKFGIDALVVIGGDGSYHGALRLTEHGYNTIGLPGTIDNDIPFTDFTIGFDTALNTAVDAIDKIRDTAKSHQRVFAVQVMGRNAADIALWAGVASGADAVIAPGFDYDVEAIANKLKKNRANGKDYGIIVIAEGDANSDAAPEFIDQLKQYGDFDARATVIGHVQRGGVPSAKDRVLASKMGAYAVELL\"\n",
    "\n",
    "train_labels[train_sentences == sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RIGIMTSGGDAPGMNLAIRAVARKALSSGLEAYGINYGFAGLVAGD...\n",
       "Name: sequence, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[train_sentences == sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9Pygw9NmYC2",
    "outputId": "809ada4e-e374-4ef9-a5d1-5122e614cdf3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          PF00365\n",
       "4          PF01237\n",
       "7          PF13277\n",
       "9          PF00831\n",
       "10         PF00560\n",
       "            ...   \n",
       "1086720    PF13649\n",
       "1086724    PF01890\n",
       "1086729    PF13004\n",
       "1086735    PF04066\n",
       "1086738    PF14748\n",
       "Name: family_accession, Length: 439493, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YpwLEWxTUeQp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pfam_famililes = set(train_labels.values)\n",
    "\n",
    "with open(\"pfam_families.txt\", \"w\") as f:\n",
    "  for pfam_family in pfam_famililes:\n",
    "    f.write(pfam_family + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxKz_vnFcAY0",
    "outputId": "44eb9e21-59e4-47ae-e304-af6a41736287",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439493/439493 [00:00<00:00, 2535552.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "\n",
    "for v in tqdm.tqdm(train_labels):\n",
    "  labels.add(v)\n",
    "\n",
    "label_to_id = {k:v for v,k in enumerate(labels)}\n",
    "\n",
    "print()\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1OAo_FKgh5cr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"pfam_to_id.json\", \"w\") as f:\n",
    "  json.dump(label_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          PF00365\n",
       "4          PF01237\n",
       "7          PF13277\n",
       "9          PF00831\n",
       "10         PF00560\n",
       "            ...   \n",
       "1086720    PF13649\n",
       "1086724    PF01890\n",
       "1086729    PF13004\n",
       "1086735    PF04066\n",
       "1086738    PF14748\n",
       "Name: family_accession, Length: 439493, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U0u9LshmcikE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels_as_int = train_labels.map(label_to_id)\n",
    "val_labels_as_int = validation_labels.map(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSFxP3eydihN",
    "outputId": "1b78c384-26fe-4f58-b7b0-e8c6001dcf48",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_labels_as_int.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YA_6yOe3bxML",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\"sentence\": train_sentences, \"labels\": train_labels_as_int})\n",
    "dataset_val = Dataset.from_dict({\"sentence\": validation_sentences, \"labels\": val_labels_as_int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASspYuGYeDer",
    "outputId": "4646db72-0efd-4c3a-bb7c-3cd9b6e190df",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'labels'],\n",
       "    num_rows: 439493\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hL3JiLZ_YJMs",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "\n",
    "tokenizer.train_from_iterator(iter(train_sentences), vocab_size=vocab_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k63Zdb68XQEW",
    "outputId": "553c521d-87ed-4d48-a763-88bea5acd656",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PROTNAME_tok/vocab.json', 'PROTNAME_tok/merges.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir {tokenizer_folder}\n",
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTLXACRuXpIc",
    "outputId": "c8829278-6a6f-4357-c8dd-cb45585af575",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapoint : RIGIMTSGGDAPGMNLAIRAVARKALSSGLEAYGINYGFAGLVAGDIHEFKAADLDDMVSQGGTMLYSARYPEFAQEESQLKGIEQLKKFGIDALVVIGGDGSYHGALRLTEHGYNTIGLPGTIDNDIPFTDFTIGFDTALNTAVDAIDKIRDTAKSHQRVFAVQVMGRNAADIALWAGVASGADAVIAPGFDYDVEAIANKLKKNRANGKDYGIIVIAEGDANSDAAPEFIDQLKQYGDFDARATVIGHVQRGGVPSAKDRVLASKMGAYAVELL, len(276)\n",
      "Encoding(num_tokens=278, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "datapoint = train_sentences.iloc[0]\n",
    "encoded = tokenizer.encode(datapoint) \n",
    "print(f\"datapoint : {datapoint}, len({len(datapoint)})\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYmIxnH_jsH4",
    "outputId": "9d93146d-c0c9-491f-86e7-815c3c1acbde",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 54, 45, 43, 45, 49, 56, 55, 43, 43, 40, 37, 52, 43, 49, 50, 48, 37, 45, 54, 37, 58, 37, 54, 47, 37, 48, 55, 55, 43, 48, 41, 37, 61, 43, 45, 50, 61, 43, 42, 37, 43, 48, 58, 37, 43, 40, 45, 44, 41, 42, 47, 37, 37, 40, 48, 40, 40, 49, 58, 55, 53, 43, 43, 56, 49, 48, 61, 55, 37, 54, 61, 52, 41, 42, 37, 53, 41, 41, 55, 53, 48, 47, 43, 45, 41, 53, 48, 47, 47, 42, 43, 45, 40, 37, 48, 58, 58, 45, 43, 43, 40, 43, 55, 61, 44, 43, 37, 48, 54, 48, 56, 41, 44, 43, 61, 50, 56, 45, 43, 48, 52, 43, 56, 45, 40, 50, 40, 45, 52, 42, 56, 40, 42, 56, 45, 43, 42, 40, 56, 37, 48, 50, 56, 37, 58, 40, 37, 45, 40, 47, 45, 54, 40, 56, 37, 47, 55, 44, 53, 54, 58, 42, 37, 58, 53, 58, 49, 43, 54, 50, 37, 37, 40, 45, 37, 48, 59, 37, 43, 58, 37, 55, 43, 37, 40, 37, 58, 45, 37, 52, 43, 42, 40, 61, 40, 58, 41, 37, 45, 37, 50, 47, 48, 47, 47, 50, 54, 37, 50, 43, 47, 40, 61, 43, 45, 45, 58, 45, 37, 41, 43, 40, 37, 50, 55, 40, 37, 37, 52, 41, 42, 45, 40, 53, 48, 47, 53, 61, 43, 40, 42, 40, 37, 54, 37, 56, 58, 45, 43, 44, 58, 53, 54, 43, 43, 58, 52, 55, 37, 47, 40, 54, 58, 48, 37, 55, 47, 49, 43, 37, 61, 37, 58, 41, 48, 48, 2]\n"
     ]
    }
   ],
   "source": [
    "print(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EeA5HK8BhHOx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OljxIdeYkKu",
    "outputId": "dff194c4-4aaa-4879-a614-6378549e3cf2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Mon Apr 24 11:09:14 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  Off |\n",
      "|  0%   29C    P8    24W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi\n",
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "f149199b394c43de8126af3ef5e41d66",
      "77c133619f5e4396b28c1546afe9bf3f",
      "a26e1731b8164026a17df16ecf72cc67",
      "06cc5c43eb2c432f987b0069d138e24e",
      "059b6ccd56194ccb9b64d599184497f1",
      "a1cca917a96843b386c410a0382b5b66",
      "66918ac5873d4d6e870b10544f7116f0",
      "475a94913f744dd38ed527c644937c5b",
      "19565f4ed23c41159482185bc0be514a",
      "812ec33a5bdf47d08c168f7216d71c4c",
      "85f63ba15c1849d6b381a7f93c1834d5",
      "50f20bfac1ef45a79485574a2d0f2c12",
      "75d8671f5b594f0b87cb1b40cd88a90c",
      "694dcf06b76248fb880ea7e6a37836b6",
      "0ebd134e2bb649af9e5dbba8f3470f41",
      "25eeea1efb1d4c12a2af13b333982b20",
      "6b384bf8e37b41fd9554de49c2aa9729",
      "8f2daa08824646e68caae3aea4248ebd",
      "98532a8f8d8a4461977abb27c588ff43",
      "df05a94d653943ad9cb40274ba4bc7e6",
      "0c0fc1433bf7484bb32b50da56fb4635",
      "a83817e308004a409032a45dde05ce08"
     ]
    },
    "id": "Nbc3ik9Hplvu",
    "outputId": "69a559b8-74e7-482e-e531-fe3174cc6cc4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/439493 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=max_length, padding=True)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset_val = dataset_val.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VseUNi5MZh7P",
    "outputId": "82741b31-dd62-424b-904e-125802d1c28b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=max_length+2,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=8,\n",
    "    type_vocab_size=1,\n",
    "    num_labels=n_families_of_interest+1,\n",
    "    label2idx=label_to_id\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxR65OMXjwbO",
    "outputId": "da78fe3f-8ae6-4237-ecbd-d6f2fe629638",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PF17802': 0, 'PF05013': 1, 'PF18821': 2, 'PF18075': 3, 'PF04039': 4, 'PF06130': 5, 'PF02654': 6, 'PF00163': 7, 'PF00719': 8, 'PF06165': 9, 'PF17921': 10, 'PF00376': 11, 'PF02466': 12, 'PF00237': 13, 'PF05649': 14, 'PF06094': 15, 'PF01369': 16, 'PF01649': 17, 'PF01975': 18, 'PF07503': 19, 'PF00731': 20, 'PF03334': 21, 'PF01985': 22, 'PF11760': 23, 'PF01268': 24, 'PF04055': 25, 'PF08712': 26, 'PF00233': 27, 'PF02537': 28, 'PF04122': 29, 'PF03619': 30, 'PF08148': 31, 'PF02739': 32, 'PF03323': 33, 'PF04237': 34, 'PF10436': 35, 'PF06421': 36, 'PF04253': 37, 'PF01657': 38, 'PF02545': 39, 'PF02811': 40, 'PF00396': 41, 'PF00557': 42, 'PF07884': 43, 'PF00312': 44, 'PF03862': 45, 'PF01339': 46, 'PF17757': 47, 'PF09269': 48, 'PF03449': 49, 'PF02686': 50, 'PF02777': 51, 'PF01867': 52, 'PF17202': 53, 'PF01193': 54, 'PF01904': 55, 'PF08486': 56, 'PF06965': 57, 'PF14805': 58, 'PF16124': 59, 'PF14403': 60, 'PF02881': 61, 'PF13565': 62, 'PF02625': 63, 'PF02436': 64, 'PF13793': 65, 'PF08393': 66, 'PF00200': 67, 'PF07804': 68, 'PF02383': 69, 'PF08299': 70, 'PF06580': 71, 'PF00484': 72, 'PF02152': 73, 'PF16353': 74, 'PF04892': 75, 'PF17762': 76, 'PF06724': 77, 'PF17853': 78, 'PF00515': 79, 'PF00278': 80, 'PF14508': 81, 'PF02817': 82, 'PF06719': 83, 'PF03098': 84, 'PF02563': 85, 'PF04519': 86, 'PF05383': 87, 'PF01253': 88, 'PF15901': 89, 'PF04675': 90, 'PF00490': 91, 'PF04316': 92, 'PF03613': 93, 'PF12399': 94, 'PF01250': 95, 'PF16123': 96, 'PF01484': 97, 'PF02547': 98, 'PF05201': 99, 'PF02632': 100, 'PF01226': 101, 'PF01237': 102, 'PF08338': 103, 'PF01368': 104, 'PF17912': 105, 'PF00988': 106, 'PF04402': 107, 'PF10431': 108, 'PF02953': 109, 'PF00353': 110, 'PF13732': 111, 'PF01386': 112, 'PF00381': 113, 'PF01197': 114, 'PF02887': 115, 'PF03952': 116, 'PF03733': 117, 'PF00766': 118, 'PF04296': 119, 'PF00858': 120, 'PF04371': 121, 'PF14905': 122, 'PF01799': 123, 'PF03457': 124, 'PF00100': 125, 'PF01116': 126, 'PF04367': 127, 'PF03015': 128, 'PF04205': 129, 'PF01281': 130, 'PF14450': 131, 'PF00709': 132, 'PF13207': 133, 'PF03116': 134, 'PF05960': 135, 'PF00281': 136, 'PF03776': 137, 'PF05226': 138, 'PF17871': 139, 'PF00687': 140, 'PF06245': 141, 'PF02646': 142, 'PF16863': 143, 'PF02583': 144, 'PF01556': 145, 'PF06686': 146, 'PF01035': 147, 'PF02618': 148, 'PF13517': 149, 'PF13537': 150, 'PF16859': 151, 'PF00327': 152, 'PF16199': 153, 'PF00586': 154, 'PF04043': 155, 'PF02260': 156, 'PF01957': 157, 'PF00288': 158, 'PF08542': 159, 'PF02457': 160, 'PF07332': 161, 'PF13408': 162, 'PF01970': 163, 'PF07538': 164, 'PF04973': 165, 'PF04003': 166, 'PF01642': 167, 'PF05191': 168, 'PF07703': 169, 'PF13288': 170, 'PF13561': 171, 'PF03947': 172, 'PF06026': 173, 'PF05552': 174, 'PF03443': 175, 'PF02601': 176, 'PF07486': 177, 'PF01813': 178, 'PF02491': 179, 'PF04377': 180, 'PF09362': 181, 'PF00734': 182, 'PF13475': 183, 'PF12874': 184, 'PF17764': 185, 'PF03719': 186, 'PF00885': 187, 'PF05257': 188, 'PF03315': 189, 'PF01264': 190, 'PF01255': 191, 'PF00013': 192, 'PF00241': 193, 'PF04117': 194, 'PF02391': 195, 'PF01139': 196, 'PF02673': 197, 'PF02445': 198, 'PF13445': 199, 'PF13472': 200, 'PF13850': 201, 'PF04024': 202, 'PF04390': 203, 'PF16653': 204, 'PF00468': 205, 'PF05161': 206, 'PF01149': 207, 'PF11799': 208, 'PF12344': 209, 'PF01302': 210, 'PF04810': 211, 'PF09723': 212, 'PF16016': 213, 'PF16901': 214, 'PF03830': 215, 'PF00037': 216, 'PF00771': 217, 'PF08766': 218, 'PF12775': 219, 'PF00902': 220, 'PF03479': 221, 'PF14278': 222, 'PF10150': 223, 'PF01702': 224, 'PF00238': 225, 'PF02322': 226, 'PF03740': 227, 'PF16488': 228, 'PF09397': 229, 'PF01221': 230, 'PF08310': 231, 'PF02844': 232, 'PF02518': 233, 'PF03484': 234, 'PF00813': 235, 'PF00593': 236, 'PF00300': 237, 'PF02609': 238, 'PF01798': 239, 'PF00936': 240, 'PF00252': 241, 'PF17676': 242, 'PF04229': 243, 'PF13570': 244, 'PF02787': 245, 'PF01510': 246, 'PF07971': 247, 'PF06172': 248, 'PF02582': 249, 'PF06071': 250, 'PF03797': 251, 'PF10135': 252, 'PF01722': 253, 'PF12804': 254, 'PF14319': 255, 'PF03796': 256, 'PF17940': 257, 'PF00338': 258, 'PF00673': 259, 'PF00149': 260, 'PF00699': 261, 'PF03466': 262, 'PF17803': 263, 'PF02277': 264, 'PF13305': 265, 'PF04413': 266, 'PF00019': 267, 'PF02367': 268, 'PF01790': 269, 'PF04515': 270, 'PF13634': 271, 'PF13340': 272, 'PF02571': 273, 'PF05697': 274, 'PF00842': 275, 'PF02570': 276, 'PF07811': 277, 'PF01220': 278, 'PF00333': 279, 'PF00189': 280, 'PF16326': 281, 'PF05485': 282, 'PF06961': 283, 'PF07927': 284, 'PF02482': 285, 'PF14510': 286, 'PF02511': 287, 'PF00887': 288, 'PF01239': 289, 'PF01367': 290, 'PF12911': 291, 'PF13004': 292, 'PF13559': 293, 'PF02194': 294, 'PF01944': 295, 'PF02569': 296, 'PF00745': 297, 'PF17678': 298, 'PF03028': 299, 'PF06912': 300, 'PF00681': 301, 'PF01336': 302, 'PF10017': 303, 'PF00146': 304, 'PF17910': 305, 'PF04961': 306, 'PF13802': 307, 'PF01259': 308, 'PF13742': 309, 'PF02771': 310, 'PF02690': 311, 'PF01161': 312, 'PF03705': 313, 'PF10728': 314, 'PF05198': 315, 'PF12464': 316, 'PF02978': 317, 'PF18074': 318, 'PF07497': 319, 'PF04430': 320, 'PF03124': 321, 'PF03372': 322, 'PF01219': 323, 'PF04298': 324, 'PF00115': 325, 'PF16220': 326, 'PF00624': 327, 'PF14791': 328, 'PF03134': 329, 'PF04815': 330, 'PF02863': 331, 'PF03609': 332, 'PF17852': 333, 'PF01687': 334, 'PF03948': 335, 'PF08669': 336, 'PF01832': 337, 'PF01168': 338, 'PF02244': 339, 'PF07004': 340, 'PF02607': 341, 'PF03748': 342, 'PF02503': 343, 'PF02592': 344, 'PF06422': 345, 'PF14031': 346, 'PF05494': 347, 'PF01040': 348, 'PF04327': 349, 'PF09527': 350, 'PF13280': 351, 'PF09719': 352, 'PF02965': 353, 'PF17759': 354, 'PF00830': 355, 'PF02502': 356, 'PF03914': 357, 'PF13597': 358, 'PF02660': 359, 'PF00330': 360, 'PF00662': 361, 'PF08487': 362, 'PF13959': 363, 'PF00343': 364, 'PF01741': 365, 'PF03808': 366, 'PF16177': 367, 'PF01195': 368, 'PF02559': 369, 'PF02548': 370, 'PF01196': 371, 'PF14490': 372, 'PF03727': 373, 'PF04408': 374, 'PF04255': 375, 'PF04015': 376, 'PF13396': 377, 'PF04461': 378, 'PF01679': 379, 'PF14815': 380, 'PF02207': 381, 'PF01535': 382, 'PF13545': 383, 'PF03073': 384, 'PF01940': 385, 'PF10589': 386, 'PF00571': 387, 'PF02225': 388, 'PF02138': 389, 'PF13774': 390, 'PF05025': 391, 'PF00162': 392, 'PF00393': 393, 'PF08494': 394, 'PF13356': 395, 'PF00194': 396, 'PF02880': 397, 'PF01430': 398, 'PF02424': 399, 'PF08522': 400, 'PF16212': 401, 'PF13234': 402, 'PF00617': 403, 'PF11929': 404, 'PF02572': 405, 'PF02733': 406, 'PF02843': 407, 'PF09383': 408, 'PF09118': 409, 'PF06441': 410, 'PF02132': 411, 'PF04982': 412, 'PF01066': 413, 'PF13376': 414, 'PF01106': 415, 'PF00920': 416, 'PF07735': 417, 'PF07664': 418, 'PF00572': 419, 'PF12937': 420, 'PF10385': 421, 'PF02590': 422, 'PF01018': 423, 'PF02538': 424, 'PF00231': 425, 'PF17876': 426, 'PF08044': 427, 'PF02729': 428, 'PF01392': 429, 'PF02657': 430, 'PF01027': 431, 'PF02405': 432, 'PF00253': 433, 'PF08345': 434, 'PF01329': 435, 'PF00883': 436, 'PF18759': 437, 'PF10369': 438, 'PF13920': 439, 'PF01730': 440, 'PF05949': 441, 'PF02698': 442, 'PF13508': 443, 'PF04248': 444, 'PF01926': 445, 'PF17042': 446, 'PF00380': 447, 'PF00344': 448, 'PF13277': 449, 'PF04960': 450, 'PF09989': 451, 'PF04860': 452, 'PF03938': 453, 'PF08436': 454, 'PF01165': 455, 'PF18766': 456, 'PF00888': 457, 'PF00476': 458, 'PF09363': 459, 'PF13927': 460, 'PF00006': 461, 'PF10035': 462, 'PF03747': 463, 'PF12806': 464, 'PF01618': 465, 'PF18198': 466, 'PF00611': 467, 'PF02375': 468, 'PF02256': 469, 'PF01357': 470, 'PF00348': 471, 'PF00391': 472, 'PF02410': 473, 'PF00400': 474, 'PF01043': 475, 'PF01773': 476, 'PF09365': 477, 'PF03588': 478, 'PF04066': 479, 'PF00815': 480, 'PF00397': 481, 'PF03489': 482, 'PF03458': 483, 'PF01176': 484, 'PF11975': 485, 'PF02643': 486, 'PF07261': 487, 'PF01765': 488, 'PF03645': 489, 'PF03618': 490, 'PF13682': 491, 'PF14416': 492, 'PF02020': 493, 'PF10646': 494, 'PF01634': 495, 'PF01052': 496, 'PF18803': 497, 'PF05598': 498, 'PF00755': 499, 'PF02308': 500, 'PF04536': 501, 'PF01817': 502, 'PF07479': 503, 'PF06541': 504, 'PF03352': 505, 'PF01992': 506, 'PF03481': 507, 'PF03030': 508, 'PF01652': 509, 'PF03626': 510, 'PF05235': 511, 'PF09371': 512, 'PF01311': 513, 'PF00365': 514, 'PF16193': 515, 'PF04087': 516, 'PF06050': 517, 'PF09285': 518, 'PF02517': 519, 'PF10442': 520, 'PF12878': 521, 'PF00472': 522, 'PF12704': 523, 'PF00104': 524, 'PF10557': 525, 'PF16188': 526, 'PF00140': 527, 'PF12686': 528, 'PF03140': 529, 'PF01450': 530, 'PF08516': 531, 'PF04366': 532, 'PF01820': 533, 'PF00560': 534, 'PF06477': 535, 'PF04092': 536, 'PF10509': 537, 'PF06968': 538, 'PF16886': 539, 'PF00499': 540, 'PF13556': 541, 'PF07907': 542, 'PF03331': 543, 'PF02415': 544, 'PF04314': 545, 'PF02594': 546, 'PF01706': 547, 'PF14226': 548, 'PF02773': 549, 'PF01169': 550, 'PF08711': 551, 'PF04241': 552, 'PF00314': 553, 'PF04294': 554, 'PF00995': 555, 'PF00137': 556, 'PF17137': 557, 'PF08029': 558, 'PF02397': 559, 'PF02671': 560, 'PF04002': 561, 'PF01384': 562, 'PF05402': 563, 'PF13012': 564, 'PF01888': 565, 'PF01055': 566, 'PF03150': 567, 'PF04127': 568, 'PF07264': 569, 'PF00177': 570, 'PF16325': 571, 'PF17827': 572, 'PF13490': 573, 'PF01245': 574, 'PF12697': 575, 'PF18199': 576, 'PF07593': 577, 'PF17768': 578, 'PF02021': 579, 'PF14497': 580, 'PF02575': 581, 'PF00646': 582, 'PF03561': 583, 'PF10590': 584, 'PF05504': 585, 'PF12396': 586, 'PF01725': 587, 'PF01502': 588, 'PF01455': 589, 'PF00576': 590, 'PF13493': 591, 'PF12781': 592, 'PF13089': 593, 'PF01155': 594, 'PF04264': 595, 'PF16320': 596, 'PF12680': 597, 'PF07963': 598, 'PF00271': 599, 'PF10825': 600, 'PF00119': 601, 'PF00433': 602, 'PF03473': 603, 'PF02775': 604, 'PF03746': 605, 'PF02446': 606, 'PF01313': 607, 'PF03861': 608, 'PF02934': 609, 'PF03461': 610, 'PF01330': 611, 'PF02033': 612, 'PF00040': 613, 'PF04299': 614, 'PF00298': 615, 'PF02669': 616, 'PF01924': 617, 'PF07549': 618, 'PF04051': 619, 'PF17941': 620, 'PF05635': 621, 'PF02595': 622, 'PF03462': 623, 'PF12661': 624, 'PF01590': 625, 'PF04463': 626, 'PF02677': 627, 'PF10415': 628, 'PF07719': 629, 'PF01379': 630, 'PF14237': 631, 'PF13368': 632, 'PF01546': 633, 'PF09754': 634, 'PF03120': 635, 'PF13405': 636, 'PF00329': 637, 'PF02603': 638, 'PF14842': 639, 'PF01424': 640, 'PF01132': 641, 'PF13399': 642, 'PF01977': 643, 'PF00438': 644, 'PF02542': 645, 'PF03367': 646, 'PF03381': 647, 'PF12002': 648, 'PF05336': 649, 'PF02626': 650, 'PF16355': 651, 'PF03595': 652, 'PF00276': 653, 'PF16658': 654, 'PF02820': 655, 'PF00475': 656, 'PF00003': 657, 'PF05524': 658, 'PF08495': 659, 'PF11987': 660, 'PF09347': 661, 'PF04168': 662, 'PF00020': 663, 'PF02508': 664, 'PF17146': 665, 'PF02578': 666, 'PF00828': 667, 'PF02016': 668, 'PF00221': 669, 'PF14748': 670, 'PF02622': 671, 'PF04472': 672, 'PF14310': 673, 'PF18072': 674, 'PF01458': 675, 'PF03636': 676, 'PF01156': 677, 'PF00684': 678, 'PF13428': 679, 'PF04172': 680, 'PF07873': 681, 'PF10410': 682, 'PF08439': 683, 'PF02245': 684, 'PF16870': 685, 'PF02650': 686, 'PF01967': 687, 'PF01887': 688, 'PF00447': 689, 'PF02873': 690, 'PF01523': 691, 'PF13449': 692, 'PF00542': 693, 'PF16192': 694, 'PF01981': 695, 'PF13649': 696, 'PF00926': 697, 'PF18741': 698, 'PF14841': 699, 'PF02201': 700, 'PF03928': 701, 'PF06133': 702, 'PF12833': 703, 'PF01960': 704, 'PF09479': 705, 'PF04403': 706, 'PF04085': 707, 'PF03483': 708, 'PF14698': 709, 'PF01179': 710, 'PF17763': 711, 'PF13167': 712, 'PF00831': 713, 'PF00203': 714, 'PF01871': 715, 'PF17801': 716, 'PF01288': 717, 'PF06750': 718, 'PF02514': 719, 'PF16189': 720, 'PF00023': 721, 'PF04325': 722, 'PF02990': 723, 'PF17755': 724, 'PF04551': 725, 'PF01088': 726, 'PF02833': 727, 'PF06949': 728, 'PF02104': 729, 'PF13335': 730, 'PF07556': 731, 'PF09827': 732, 'PF04376': 733, 'PF05195': 734, 'PF00478': 735, 'PF00358': 736, 'PF04145': 737, 'PF01632': 738, 'PF13378': 739, 'PF07516': 740, 'PF13660': 741, 'PF01514': 742, 'PF17957': 743, 'PF01628': 744, 'PF14008': 745, 'PF00886': 746, 'PF03186': 747, 'PF02742': 748, 'PF04304': 749, 'PF02561': 750, 'PF01016': 751, 'PF16875': 752, 'PF04020': 753, 'PF00880': 754, 'PF01312': 755, 'PF01923': 756, 'PF08331': 757, 'PF06628': 758, 'PF01504': 759, 'PF02096': 760, 'PF03937': 761, 'PF01825': 762, 'PF01346': 763, 'PF12951': 764, 'PF11838': 765, 'PF13676': 766, 'PF00479': 767, 'PF00829': 768, 'PF02633': 769, 'PF13975': 770, 'PF13640': 771, 'PF00250': 772, 'PF18076': 773, 'PF03840': 774, 'PF00309': 775, 'PF06022': 776, 'PF02699': 777, 'PF07494': 778, 'PF04199': 779, 'PF18758': 780, 'PF03975': 781, 'PF01416': 782, 'PF05792': 783, 'PF12392': 784, 'PF08529': 785, 'PF01899': 786, 'PF01625': 787, 'PF02417': 788, 'PF07525': 789, 'PF05033': 790, 'PF10397': 791, 'PF03707': 792, 'PF10518': 793, 'PF08459': 794, 'PF08592': 795, 'PF03600': 796, 'PF02912': 797, 'PF00174': 798, 'PF03625': 799, 'PF12631': 800, 'PF10601': 801, 'PF01641': 802, 'PF04032': 803, 'PF04166': 804, 'PF11915': 805, 'PF00584': 806, 'PF18052': 807, 'PF00268': 808, 'PF13439': 809, 'PF02261': 810, 'PF07743': 811, 'PF08376': 812, 'PF02812': 813, 'PF02929': 814, 'PF03990': 815, 'PF00095': 816, 'PF04613': 817, 'PF01352': 818, 'PF03993': 819, 'PF04963': 820, 'PF02365': 821, 'PF01774': 822, 'PF03441': 823, 'PF05658': 824, 'PF02092': 825, 'PF05173': 826, 'PF03453': 827, 'PF13677': 828, 'PF13460': 829, 'PF06418': 830, 'PF00349': 831, 'PF13894': 832, 'PF01266': 833, 'PF01084': 834, 'PF03775': 835, 'PF13720': 836, 'PF00471': 837, 'PF02738': 838, 'PF01808': 839, 'PF04427': 840, 'PF14693': 841, 'PF00401': 842, 'PF18345': 843, 'PF00285': 844, 'PF17136': 845, 'PF02700': 846, 'PF06201': 847, 'PF04749': 848, 'PF00707': 849, 'PF06429': 850, 'PF00507': 851, 'PF17963': 852, 'PF04468': 853, 'PF10576': 854, 'PF01925': 855, 'PF03478': 856, 'PF01894': 857, 'PF16321': 858, 'PF01388': 859, 'PF07075': 860, 'PF00126': 861, 'PF00908': 862, 'PF01805': 863, 'PF01252': 864, 'PF06415': 865, 'PF02401': 866, 'PF05681': 867, 'PF03880': 868, 'PF10588': 869, 'PF02233': 870, 'PF03255': 871, 'PF13976': 872, 'PF04333': 873, 'PF02574': 874, 'PF02772': 875, 'PF07331': 876, 'PF00303': 877, 'PF00346': 878, 'PF00453': 879, 'PF16916': 880, 'PF01192': 881, 'PF05184': 882, 'PF08378': 883, 'PF12019': 884, 'PF10143': 885, 'PF01988': 886, 'PF03788': 887, 'PF00486': 888, 'PF00352': 889, 'PF14714': 890, 'PF02805': 891, 'PF13382': 892, 'PF16486': 893, 'PF04060': 894, 'PF07500': 895, 'PF02885': 896, 'PF10396': 897, 'PF13478': 898, 'PF03988': 899, 'PF14804': 900, 'PF01769': 901, 'PF00596': 902, 'PF01428': 903, 'PF09995': 904, 'PF01809': 905, 'PF00199': 906, 'PF00410': 907, 'PF01715': 908, 'PF00759': 909, 'PF00986': 910, 'PF06689': 911, 'PF01804': 912, 'PF09298': 913, 'PF10996': 914, 'PF13692': 915, 'PF00925': 916, 'PF17919': 917, 'PF03119': 918, 'PF06803': 919, 'PF02910': 920, 'PF03799': 921, 'PF02586': 922, 'PF12673': 923, 'PF01693': 924, 'PF08742': 925, 'PF01987': 926, 'PF16344': 927, 'PF07715': 928, 'PF00763': 929, 'PF04000': 930, 'PF07501': 931, 'PF04295': 932, 'PF03755': 933, 'PF01713': 934, 'PF01724': 935, 'PF01205': 936, 'PF02615': 937, 'PF02580': 938, 'PF01668': 939, 'PF01990': 940, 'PF02325': 941, 'PF04547': 942, 'PF16113': 943, 'PF17862': 944, 'PF14667': 945, 'PF16874': 946, 'PF03591': 947, 'PF02781': 948, 'PF08818': 949, 'PF00036': 950, 'PF00742': 951, 'PF13620': 952, 'PF02985': 953, 'PF08497': 954, 'PF04234': 955, 'PF17517': 956, 'PF04149': 957, 'PF12662': 958, 'PF07562': 959, 'PF09994': 960, 'PF16491': 961, 'PF03929': 962, 'PF07662': 963, 'PF00984': 964, 'PF04777': 965, 'PF03091': 966, 'PF09369': 967, 'PF14420': 968, 'PF04898': 969, 'PF08379': 970, 'PF01654': 971, 'PF08340': 972, 'PF01297': 973, 'PF03946': 974, 'PF03055': 975, 'PF01709': 976, 'PF02730': 977, 'PF03729': 978, 'PF01969': 979, 'PF03313': 980, 'PF05164': 981, 'PF00677': 982, 'PF00367': 983, 'PF01980': 984, 'PF00080': 985, 'PF00318': 986, 'PF05192': 987, 'PF13244': 988, 'PF13932': 989, 'PF02465': 990, 'PF09851': 991, 'PF01890': 992, 'PF00387': 993, 'PF03883': 994, 'PF16360': 995, 'PF03737': 996, 'PF00366': 997, 'PF09180': 998, 'PF16640': 999}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(261, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(514, 128)\n",
       "      (token_type_embeddings): Embedding(1, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=1001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1ryKkoebCBS",
    "outputId": "d2f4744e-f1ed-4741-b7b8-8315ce59ba04",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8669214248657227"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters() / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "GpJ4-Sipn-VS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MwtPm4NbLWR",
    "outputId": "cd098e88-26f8-494a-acdc-4bb5f5491393",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/PROTNAME is already a clone of https://huggingface.co/mlewand/PROTBERT-tiny. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28327' max='34340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28327/34340 1:04:31 < 13:41, 7.32 it/s, Epoch 8.25/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.685800</td>\n",
       "      <td>3.452489</td>\n",
       "      <td>0.187245</td>\n",
       "      <td>0.109976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.440100</td>\n",
       "      <td>3.175666</td>\n",
       "      <td>0.239398</td>\n",
       "      <td>0.163652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.205800</td>\n",
       "      <td>3.005582</td>\n",
       "      <td>0.264795</td>\n",
       "      <td>0.194011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.049600</td>\n",
       "      <td>2.804687</td>\n",
       "      <td>0.312847</td>\n",
       "      <td>0.237627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.937200</td>\n",
       "      <td>2.730087</td>\n",
       "      <td>0.316672</td>\n",
       "      <td>0.243396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.840000</td>\n",
       "      <td>2.626378</td>\n",
       "      <td>0.343963</td>\n",
       "      <td>0.271696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.759600</td>\n",
       "      <td>2.541703</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.297097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.688800</td>\n",
       "      <td>2.499723</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>0.309842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 29\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39msave_folder,\n\u001b[1;32m      5\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     bf16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2717\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=os.path.join(save_folder, \"logs\"),            # directory for storing logs\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"mlewand/PROTBERT-tiny\",\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/PROTNAME is already a clone of https://huggingface.co/mlewand/PROTBERT-tiny. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17170' max='17170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17170/17170 39:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.150300</td>\n",
       "      <td>0.799259</td>\n",
       "      <td>0.804241</td>\n",
       "      <td>0.795231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.631554</td>\n",
       "      <td>0.849314</td>\n",
       "      <td>0.843460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.476182</td>\n",
       "      <td>0.888852</td>\n",
       "      <td>0.885345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.386687</td>\n",
       "      <td>0.910718</td>\n",
       "      <td>0.908633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>0.338941</td>\n",
       "      <td>0.923020</td>\n",
       "      <td>0.921561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17170, training_loss=0.7498414721125145, metrics={'train_runtime': 2342.788, 'train_samples_per_second': 937.97, 'train_steps_per_second': 7.329, 'total_flos': 1.2416350270311936e+16, 'train_loss': 0.7498414721125145, 'epoch': 5.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=os.path.join(save_folder, \"logs\"),            # directory for storing logs\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"mlewand/PROTBERT-tiny\",\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFK63TLWRF-H",
    "outputId": "b8adca37-67f3-4d0f-8a70-534dda8a8da3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=save_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=os.path.join(save_folder, \"logs\"),            # directory for storing logs\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.label2id = label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.id2label = {v:k for k,v in label_to_id.items()}\n",
    "model.config.id2label[1000] = \"unk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "oSwC6feURF-H",
    "outputId": "79c58b49-67bf-445d-c049-d36174ef4465",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mlewand/PROTBERT-tiny/commit/87cbcabb67189ffbf6ba54101e1ef818736baaf5', commit_message='Upload BertForSequenceClassification', commit_description='', oid='87cbcabb67189ffbf6ba54101e1ef818736baaf5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"mlewand/PROTBERT-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Htg5OI3ARF-H",
    "outputId": "b253cc7c-dc30-4f36-f73d-04634d6c5f50",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapoint : RIGIMTSGGDAPGMNLAIRAVARKALSSGLEAYGINYGFAGLVAGDIHEFKAADLDDMVSQGGTMLYSARYPEFAQEESQLKGIEQLKKFGIDALVVIGGDGSYHGALRLTEHGYNTIGLPGTIDNDIPFTDFTIGFDTALNTAVDAIDKIRDTAKSHQRVFAVQVMGRNAADIALWAGVASGADAVIAPGFDYDVEAIANKLKKNRANGKDYGIIVIAEGDANSDAAPEFIDQLKQYGDFDARATVIGHVQRGGVPSAKDRVLASKMGAYAVELL, len(276)\n"
     ]
    }
   ],
   "source": [
    "datapoint = train_sentences.iloc[0]\n",
    "encoded = tokenizer.encode(datapoint) \n",
    "print(f\"datapoint : {datapoint}, len({len(datapoint)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "rJil095eRF-I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model=model.to(\"cpu\"),\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'PF00365', 'score': 0.9787689447402954}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"RIGIMTSGGDAPGMNLAIRAVARKALSSGLEAYGINYGFAGLVAGDIHEFKAADLDDMVSQGGTMLYSARYPEFAQEESQLKGIEQLKKFGIDALVVIGGDGSYHGALRLTEHGYNTIGLPGTIDNDIPFTDFTIGFDTALNTAVDAIDKIRDTAKSHQRVFAVQVMGRNAADIALWAGVASGADAVIAPGFDYDVEAIANKLKKNRANGKDYGIIVIAEGDANSDAAPEFIDQLKQYGDFDARATVIGHVQRGGVPSAKDRVLASKMGAYAVELL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"pfam_families1.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(label_to_id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pfam_ids = list(label_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [05:32<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_description(id):\n",
    "    response = requests.get(f\"https://www.ebi.ac.uk/interpro/api/entry/pfam/{id}\")\n",
    "    return response.json()\n",
    "\n",
    "def get_description_mock(id):\n",
    "    return {\"metadata\" : {\"description\" : \"mock\"}}\n",
    "\n",
    "def collate_all_families(pfam_familiy_ids, mock=True):\n",
    "    dataset = {}\n",
    "    for pfam_id in tqdm.tqdm(pfam_familiy_ids):\n",
    "        if mock:\n",
    "            dataset[pfam_id] = get_description_mock(pfam_id)\n",
    "        else:\n",
    "            dataset[pfam_id] = get_description(pfam_id)    \n",
    "\n",
    "    return dataset\n",
    "\n",
    "with open(os.path.join(\".\", \"pfam_data.json\"), \"w\") as f:\n",
    "    json.dump(collate_all_families(pfam_ids, mock=False), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"pfam_data.json\")) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pfam_id_to_number = {v:k for k, v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 257051.17it/s]\n"
     ]
    }
   ],
   "source": [
    "BERT_model_idx2label = {}\n",
    "for family_id in tqdm.tqdm(pfam_ids):\n",
    "    name = data[family_id][\"metadata\"][\"name\"][\"name\"]\n",
    "    description = data[family_id][\"metadata\"][\"description\"]\n",
    "\n",
    "    # print(f\"{family_id} -> {name}, {description}\")\n",
    "    entry = f\"{name}\"\n",
    "    if description is not None:\n",
    "        desc = description[0].replace(\"<p>\", \"\").replace(\"</p>\", \"\")\n",
    "        entry = f\"{family_id}\\n{name}\\n{desc}\"\n",
    "    BERT_model_idx2label[label_to_id[family_id]] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.id2label = BERT_model_idx2label\n",
    "model.config.id2label[1000] = \"unk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mlewand/PROTBERT-tiny/commit/32d1283f41ddf69883ef7ace40785425c658810a', commit_message='Upload BertForSequenceClassification', commit_description='', oid='32d1283f41ddf69883ef7ace40785425c658810a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"mlewand/PROTBERT-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model=model.to(\"cpu\"),\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'PF00560\\nLeucine Rich Repeat\\nCAUTION: This Pfam may not find all Leucine Rich Repeats in a protein.  Leucine Rich Repeats are short sequence motifs present in a number of proteins with diverse functions and cellular locations.  These repeats are usually involved in protein-protein interactions. Each Leucine Rich Repeat is composed of a beta-alpha unit. These units form elongated non-globular structures. Leucine Rich Repeats are often flanked by cysteine rich domains.',\n",
       "  'score': 0.9977093935012817}]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"SLKFLNFAQNEFNGSIPESV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    PF00560\n",
       "Name: family_accession, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[train_sentences == \"SLKFLNFAQNEFNGSIPESV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PF00560\n",
      "Leucine Rich Repeat\n",
      "CAUTION: This Pfam may not find all Leucine Rich Repeats in a protein.  Leucine Rich Repeats are short sequence motifs present in a number of proteins with diverse functions and cellular locations.  These repeats are usually involved in protein-protein interactions. Each Leucine Rich Repeat is composed of a beta-alpha unit. These units form elongated non-globular structures. Leucine Rich Repeats are often flanked by cysteine rich domains.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    'text-classification',\n",
    "    model=\"mlewand/PROTBERT-tiny\",\n",
    "    tokenizer=\"mlewand/PROTBERT-tiny\"\n",
    ")\n",
    "print(pipe(\"SLKFLNFAQNEFNGSIPESV\")[0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "059b6ccd56194ccb9b64d599184497f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "06cc5c43eb2c432f987b0069d138e24e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_812ec33a5bdf47d08c168f7216d71c4c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_85f63ba15c1849d6b381a7f93c1834d5",
      "value": " 439000/439493 [00:48&lt;00:00, 5921.62 examples/s]"
     }
    },
    "0c0fc1433bf7484bb32b50da56fb4635": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ebd134e2bb649af9e5dbba8f3470f41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c0fc1433bf7484bb32b50da56fb4635",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a83817e308004a409032a45dde05ce08",
      "value": " 54378/54378 [00:06&lt;00:00, 8957.05 examples/s]"
     }
    },
    "19565f4ed23c41159482185bc0be514a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25eeea1efb1d4c12a2af13b333982b20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "475a94913f744dd38ed527c644937c5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50f20bfac1ef45a79485574a2d0f2c12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75d8671f5b594f0b87cb1b40cd88a90c",
       "IPY_MODEL_694dcf06b76248fb880ea7e6a37836b6",
       "IPY_MODEL_0ebd134e2bb649af9e5dbba8f3470f41"
      ],
      "layout": "IPY_MODEL_25eeea1efb1d4c12a2af13b333982b20"
     }
    },
    "66918ac5873d4d6e870b10544f7116f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "694dcf06b76248fb880ea7e6a37836b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98532a8f8d8a4461977abb27c588ff43",
      "max": 54378,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_df05a94d653943ad9cb40274ba4bc7e6",
      "value": 54378
     }
    },
    "6b384bf8e37b41fd9554de49c2aa9729": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75d8671f5b594f0b87cb1b40cd88a90c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b384bf8e37b41fd9554de49c2aa9729",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8f2daa08824646e68caae3aea4248ebd",
      "value": "Map: 100%"
     }
    },
    "77c133619f5e4396b28c1546afe9bf3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1cca917a96843b386c410a0382b5b66",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_66918ac5873d4d6e870b10544f7116f0",
      "value": "Map: 100%"
     }
    },
    "812ec33a5bdf47d08c168f7216d71c4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85f63ba15c1849d6b381a7f93c1834d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f2daa08824646e68caae3aea4248ebd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98532a8f8d8a4461977abb27c588ff43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1cca917a96843b386c410a0382b5b66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a26e1731b8164026a17df16ecf72cc67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_475a94913f744dd38ed527c644937c5b",
      "max": 439493,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_19565f4ed23c41159482185bc0be514a",
      "value": 439493
     }
    },
    "a83817e308004a409032a45dde05ce08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df05a94d653943ad9cb40274ba4bc7e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f149199b394c43de8126af3ef5e41d66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77c133619f5e4396b28c1546afe9bf3f",
       "IPY_MODEL_a26e1731b8164026a17df16ecf72cc67",
       "IPY_MODEL_06cc5c43eb2c432f987b0069d138e24e"
      ],
      "layout": "IPY_MODEL_059b6ccd56194ccb9b64d599184497f1"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
