{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jgVasYi6Pls3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.0.9\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
      "done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 7419 kB of archives.\n",
      "After this operation, 16.0 MB of additional disk space will be used.\n",
      "Get:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.3.0 [7419 kB]\n",
      "Fetched 7419 kB in 1s (5757 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 21090 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.3.0_amd64.deb ...\n",
      "Unpacking git-lfs (3.3.0) ...\n",
      "Setting up git-lfs (3.3.0) ...\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
    "!apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc1ef5f82e3471ebc37f8461b078a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbqnbvk8PqXa",
    "outputId": "ddc55f1c-7573-4180-cdf8-97edc11a1cd9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908dd23e0b4242af854aaaf679dce5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b5c2058f5b42a2a31c0c1e25d8e4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae103c0e347a45e189d34c424b56a677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/14.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e29fc5e1ef4b41a0db77b6a692c819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/5.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4161243c58f4f8ba7e027e89cb63009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759bf76147b94ff1ac32d156551c0cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79d8ebb0f844693844e3b0df95094e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/60.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import BertConfig\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mlewand/PROTBERT\")\n",
    "\"\"\"\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=512,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "model = BertForMaskedLM(config=config)\n",
    "\"\"\"\n",
    "model = BertForMaskedLM.from_pretrained(\"mlewand/PROTBERT_LM-L8_hid512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbuwm0c2QAAS",
    "outputId": "4e8deb3d-4004-41c9-8bc4-71891c18fd13",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.mask_token_id)\n",
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbaLP9Z-Vxds"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "owN00WWYVzGr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YsvIZ8JV_Vb"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9ZVZotXDWBPT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_id = '1w3IQMK3PmXH-Bq6Lt_P8wxGPvr5olrZT'\n",
    "destination = 'pfam.zip'\n",
    "hf_name = \"t5-small\"\n",
    "n_families_of_interest = 40\n",
    "data_dirpath = \"pfam\"\n",
    "aminoacid_separate_by = \"\"\n",
    "max_length = 512\n",
    "tokenizer_folder = \"PROTNAME_tok\"\n",
    "save_folder = \"PROTNAME\"\n",
    "vocab_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4BalcS-VgWj"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "loV2XBknVOe0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_size(value):\n",
    "  if value >= 1024**3:\n",
    "    return f\"{round(value / 1024**3 , 3)} GB\"\n",
    "  elif value >= 1024**2:\n",
    "    return f\"{round(value / 1024**2 , 3)} MB\"\n",
    "  elif value >= 1024:\n",
    "    return f\"{round(value / 1024 , 3)} KB\"\n",
    "  return f\"{value} B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FnxVLDEaVbx9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code taken from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "    params = { 'id' : id, 'confirm' : 1 }\n",
    "    response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    \n",
    "    pb = tqdm.tqdm(response.iter_content(CHUNK_SIZE))\n",
    "    b_total = 0\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for i,chunk in enumerate(pb):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                b_total += len(chunk)\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                  pb.set_description(f\"written : {format_size(b_total)}\")\n",
    "                i += 1\n",
    "    print(\"\\n\")\n",
    "    print(f\"saved all the data to {destination}. total size : {format_size(os.stat(destination).st_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX87OH9OVjRD"
   },
   "source": [
    "# Get the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HGEcl9LVmBn",
    "outputId": "6ec50469-baf7-4cf3-de63-9ad772e77333",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "written : 468.781 MB: : 15780it [00:11, 1429.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "saved all the data to pfam.zip. total size : 493.095 MB\n"
     ]
    }
   ],
   "source": [
    "download_file_from_google_drive(file_id, destination)\n",
    "with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "!mv random_split pfam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYCBtPLqWZx4"
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZ-6Ce6AWS55",
    "outputId": "fa5582be-7200-4252-f63d-55f547c4eb2d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset partitions:  ['train', 'test', 'random_split', 'dev']\n",
      "Dataset partition \"test\" has 126171 sequences\n",
      "Dataset partition \"dev\" has 126171 sequences\n",
      "Dataset partition \"train\" has 1086741 sequences\n",
      "how many labels : 17929\n",
      "40 classes is 4.3 portion of training data\n"
     ]
    }
   ],
   "source": [
    "def read_all_shards(partition='dev', data_dir = data_dirpath):\n",
    "    shards = []\n",
    "    for fn in os.listdir(os.path.join(data_dir, partition)):\n",
    "        with open(os.path.join(data_dir, partition, fn)) as f:\n",
    "            shards.append(pd.read_csv(f, index_col=None))\n",
    "    \n",
    "    return pd.concat(shards)\n",
    "\n",
    "def read_all_data_initial():\n",
    "  global train, test, dev, all_train_ds_size, all_test_ds_size, all_dev_ds_size\n",
    "\n",
    "  test = read_all_shards('test')\n",
    "  dev = read_all_shards('dev')\n",
    "  train = read_all_shards('train')\n",
    "\n",
    "  partitions = {'test': test, 'dev': dev, 'train': train}\n",
    "  for name, df in partitions.items():\n",
    "      print('Dataset partition \"%s\" has %d sequences' % (name, len(df)))\n",
    "\n",
    "  all_train_ds_size = len(train)\n",
    "  all_test_ds_size = len(test)\n",
    "  all_dev_ds_size = len(dev)\n",
    "\n",
    "  train.reset_index(inplace=True, drop=True)\n",
    "  dev.reset_index(inplace=True, drop=True)\n",
    "  test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "def get_cumulative(data):\n",
    "    counter = Counter(data['family_accession'])\n",
    "    print(f\"how many labels : {len(counter)}\")\n",
    "    \n",
    "    datasetSize = len(data)\n",
    "    xs = []\n",
    "    x_labels = []\n",
    "    ys = []\n",
    "\n",
    "    t = 0\n",
    "    cumulative = []\n",
    "\n",
    "    for i,(x, y) in  enumerate(counter.most_common()):\n",
    "        xs.append(i)\n",
    "        x_labels.append(x)\n",
    "        ys.append(y)\n",
    "        t += y / datasetSize\n",
    "        cumulative.append(t)\n",
    "    return cumulative\n",
    "\n",
    "\n",
    "# EXECUTION CODE\n",
    "print('Available dataset partitions: ', os.listdir(data_dirpath))\n",
    "read_all_data_initial()\n",
    "cumulative = get_cumulative(train)\n",
    "print(f\"{n_families_of_interest} classes is {100 * round( cumulative[n_families_of_interest-1],3)} portion of training data\")\n",
    "\n",
    "familiesOfInterest = train.family_accession.value_counts()[:n_families_of_interest]\n",
    "\n",
    "mask = train.family_accession.isin(familiesOfInterest.index.values)\n",
    "train = train.loc[mask,:]\n",
    "\n",
    "mask = dev.family_accession.isin(familiesOfInterest.index.values)\n",
    "dev = dev.loc[mask,:]\n",
    "\n",
    "mask = test.family_accession.isin(familiesOfInterest.index.values)\n",
    "test = test.loc[mask,:]\n",
    "\n",
    "\n",
    "################################################################################\n",
    "train_seq = train['sequence']\n",
    "dev_seq = dev['sequence']\n",
    "test_seq = test['sequence']\n",
    "\n",
    "################################################################################\n",
    "train_sentences = train_seq.apply(lambda seq: aminoacid_separate_by.join([aa for aa in seq]))\n",
    "validation_sentences = dev_seq.apply(lambda seq: aminoacid_separate_by.join([aa for aa in seq]))\n",
    "test_sentences = test_seq.apply(lambda seq: aminoacid_separate_by.join([aa for aa in seq]))\n",
    "\n",
    "################################################################################\n",
    "train_labels = train['family_accession'].apply(lambda x: x.split('.')[0])\n",
    "validation_labels = dev['family_accession'].apply(lambda x: x.split('.')[0])\n",
    "test_labels = test['family_accession'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSuEYW7GWnZS",
    "outputId": "5fccd9bf-0902-4f2c-cd01-b6a5b0e0fa99",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                                      SLKFLNFAQNEFNGSIPESV\n",
       "37         KRVIDIVFSIIGIVVLSPLMLAICIYIKISSKGNAIFSHIRLGYRG...\n",
       "62         LGYWPVLAGRVVAGDQRGRQLGFPTANLAVPPELKLPAYGVYACLA...\n",
       "103        SGAAIAALFAGMGSAKGTSIAGQAAAGVVTEDPSKFGQLLVLQLLP...\n",
       "116        FVLGLVAGSLSFGGAYTAIPFVQVEAVLKGGWLPQSVFIDCIAIGN...\n",
       "                                 ...                        \n",
       "1086628    ILDGNATAAAIKEELKERVEALKAKGITPGLGTVLVGDDPASRSYV...\n",
       "1086658    TGLIQDLGRVERVIPGGMTDLWIHTSLGAAAFELGESIAVNGACLT...\n",
       "1086682    CETCKGSGGREKAKPHDCSVCGGRGQTLRIQTVGGMLSQVTTQCAN...\n",
       "1086684    LNAVKLLARVAETRSFTQAAVSLGLTQSGLSRAIGRLEAELGVRLL...\n",
       "1086720    VLDIGCGNGKITAQIAERVPAGAVLGIDSSEQMISFASSHFAPADW...\n",
       "Name: sequence, Length: 46902, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8cEz2TUWpIh",
    "outputId": "0124f48a-f8da-494e-f82f-f01eb87e4a5b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46902/46902 [00:00<00:00, 1806489.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "\n",
    "for v in tqdm.tqdm(train_labels):\n",
    "  labels.add(v)\n",
    "\n",
    "label_to_id = {k:v for v,k in enumerate(labels)}\n",
    "\n",
    "print()\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1dGYGXxBWzsh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\"sentence\": train_sentences})\n",
    "dataset_val = Dataset.from_dict({\"sentence\": validation_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brsB7FSJXE24",
    "outputId": "552ec7e1-2fd3-45ee-e262-4c9a233d802c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence'],\n",
      "    num_rows: 46902\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentence'],\n",
      "    num_rows: 5841\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSUx5vu7ZE7o",
    "outputId": "dfc1a83b-ef77-4613-dd89-d3e2617067c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapoint : SLKFLNFAQNEFNGSIPESV, len(20)\n",
      "[0, 55, 48, 47, 42, 48, 50, 42, 37, 53, 50, 41, 42, 50, 43, 55, 45, 52, 41, 55, 58, 2]\n"
     ]
    }
   ],
   "source": [
    "datapoint = train_sentences.iloc[0]\n",
    "encoded = tokenizer.encode(datapoint)\n",
    "print(f\"datapoint : {datapoint}, len({len(datapoint)})\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FxFINv-crl_c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "c365487441c04a12aabec22d86bd4561",
      "052128a69fe746a1bd79d25e4d472063",
      "1307a4f5fe064f20a0bef031d7586a46",
      "ed9146079caa4e3baf5342d051ff2fcf",
      "c448ff0f58654c04a883c0f9296c1301",
      "195e23c2e6074fe1a4da96bc628db141",
      "3f592bd421f0466f84b2c5d37cd8b804",
      "7cdace6109dd494fbe002c3a82e39c64",
      "1e279b240dbe4150bb742dc8dec92401",
      "80c9725169524de5b1c33998e8961178",
      "ea402b4672a0417b81b2d7c4c1a49ebf",
      "726e137c6dd14657af0a8e4155ad57b1",
      "63258ae8c5cd441ea2091e8e5f4eb797",
      "84a64e0330654408a4eb6a2238437c11",
      "9bbffee00a8242098ea61e6e863bd240",
      "c680f4b4275444e3847082461f85225a",
      "32c4135d36874d72afededd170c6f050",
      "95bbd4a7fedb46088d0012e61b688ba2",
      "22d4adb90ca74c66b976dd648ea5f1b3",
      "d33e1cb843694f3681061cd0770258fb",
      "d686d428facd48da82133c0dc58c1327",
      "5f5e468ca9424cb29b7551fd7d54c3bd"
     ]
    },
    "id": "hHjjVllLrhyk",
    "outputId": "e3b0ae18-ce00-4a93-800b-2cd45cc68c66",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset_val = dataset_val.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0OHc3GSdY4L",
    "outputId": "34afe0c0-53a1-4de8-ec91-5e8e7c530d61",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 55, 48, 47, 42, 48, 50, 42,  4, 53, 50, 41, 42, 50, 43, 55, 45, 52,\n",
       "         41, 55, 58,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100,   37, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.05)\n",
    "tok = tokenizer(train_sentences.iloc[0], truncation=True)\n",
    "data_collator([tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xJz2ahLh5Eav",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  # labels = np.argmax(labels, axis=1)\n",
    "  preds = pred.predictions\n",
    "\n",
    "  tot = np.sum(labels != -100) + 1e-10\n",
    "  classified_correctly = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    mask = preds[i, :] != -100\n",
    "    classified_correctly += np.sum(preds[i, mask] == labels[i, mask])\n",
    "  \n",
    "  return {\"exact_matches\": classified_correctly / tot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Nlk_0wKVaCnu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.7.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForMaskedLM                                         --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              66,816\n",
       "│    │    └─Embedding: 3-2                              131,072\n",
       "│    │    └─Embedding: 3-3                              256\n",
       "│    │    └─LayerNorm: 3-4                              512\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             14,723,072\n",
       "├─BertOnlyMLMHead: 1-2                                  --\n",
       "│    └─BertLMPredictionHead: 2-3                        --\n",
       "│    │    └─BertPredictionHeadTransform: 3-7            66,304\n",
       "│    │    └─Linear: 3-8                                 67,077\n",
       "================================================================================\n",
       "Total params: 15,055,109\n",
       "Trainable params: 15,055,109\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "TB802tI4qpcT",
    "outputId": "4e1795d1-f061-429e-fecf-e5654ebdfac5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/mlewand/PROTBERT_LM-L8_hid512 into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f534423cff94706b3f9211b5273bb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 7.40k/57.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0404fb0699545cfa661b83dec545808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 3.50k/3.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9f7ac13aa645789d432cfff1b98f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  29%|##8       | 1.00k/3.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab89bcea5a94ec4849299e2806e28e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/57.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='107100' max='146600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [107100/146600 4:04:34 < 1:30:12, 7.30 it/s, Epoch 146.11/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.564100</td>\n",
       "      <td>2.555925</td>\n",
       "      <td>0.208697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.550600</td>\n",
       "      <td>2.530543</td>\n",
       "      <td>0.212246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.527900</td>\n",
       "      <td>2.515243</td>\n",
       "      <td>0.221396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.509000</td>\n",
       "      <td>2.488445</td>\n",
       "      <td>0.234904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.486500</td>\n",
       "      <td>2.467027</td>\n",
       "      <td>0.240221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.462600</td>\n",
       "      <td>2.441635</td>\n",
       "      <td>0.248584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.435200</td>\n",
       "      <td>2.422707</td>\n",
       "      <td>0.256869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.421700</td>\n",
       "      <td>2.384675</td>\n",
       "      <td>0.268045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.393900</td>\n",
       "      <td>2.355874</td>\n",
       "      <td>0.277235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.376900</td>\n",
       "      <td>2.337508</td>\n",
       "      <td>0.284656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.346600</td>\n",
       "      <td>2.324083</td>\n",
       "      <td>0.285522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.325300</td>\n",
       "      <td>2.299900</td>\n",
       "      <td>0.296016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.308300</td>\n",
       "      <td>2.260681</td>\n",
       "      <td>0.307925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.283500</td>\n",
       "      <td>2.250004</td>\n",
       "      <td>0.313214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.265100</td>\n",
       "      <td>2.217837</td>\n",
       "      <td>0.320235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.241500</td>\n",
       "      <td>2.216837</td>\n",
       "      <td>0.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.232200</td>\n",
       "      <td>2.193034</td>\n",
       "      <td>0.328159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.210800</td>\n",
       "      <td>2.171182</td>\n",
       "      <td>0.339963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.194700</td>\n",
       "      <td>2.170344</td>\n",
       "      <td>0.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.178200</td>\n",
       "      <td>2.146722</td>\n",
       "      <td>0.344140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.161900</td>\n",
       "      <td>2.154216</td>\n",
       "      <td>0.345479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.149000</td>\n",
       "      <td>2.104250</td>\n",
       "      <td>0.356587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.138500</td>\n",
       "      <td>2.100550</td>\n",
       "      <td>0.358393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.125600</td>\n",
       "      <td>2.090763</td>\n",
       "      <td>0.363815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.113300</td>\n",
       "      <td>2.074775</td>\n",
       "      <td>0.363908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.101500</td>\n",
       "      <td>2.075450</td>\n",
       "      <td>0.367663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.094500</td>\n",
       "      <td>2.054229</td>\n",
       "      <td>0.372063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.075700</td>\n",
       "      <td>2.046561</td>\n",
       "      <td>0.372796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.072400</td>\n",
       "      <td>2.038382</td>\n",
       "      <td>0.378914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.057800</td>\n",
       "      <td>2.022857</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.049300</td>\n",
       "      <td>2.010207</td>\n",
       "      <td>0.387171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.039200</td>\n",
       "      <td>2.016670</td>\n",
       "      <td>0.385142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.030500</td>\n",
       "      <td>2.001749</td>\n",
       "      <td>0.390222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.019900</td>\n",
       "      <td>1.979584</td>\n",
       "      <td>0.393031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.012700</td>\n",
       "      <td>1.975598</td>\n",
       "      <td>0.398128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.005000</td>\n",
       "      <td>1.953665</td>\n",
       "      <td>0.404698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.991000</td>\n",
       "      <td>1.969948</td>\n",
       "      <td>0.397756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.988100</td>\n",
       "      <td>1.958952</td>\n",
       "      <td>0.398705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.976400</td>\n",
       "      <td>1.945745</td>\n",
       "      <td>0.407835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.972300</td>\n",
       "      <td>1.926205</td>\n",
       "      <td>0.413147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.963200</td>\n",
       "      <td>1.937292</td>\n",
       "      <td>0.407939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.953600</td>\n",
       "      <td>1.924927</td>\n",
       "      <td>0.413307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.942600</td>\n",
       "      <td>1.904223</td>\n",
       "      <td>0.416161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.939300</td>\n",
       "      <td>1.903919</td>\n",
       "      <td>0.418259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.931700</td>\n",
       "      <td>1.902009</td>\n",
       "      <td>0.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.926600</td>\n",
       "      <td>1.890240</td>\n",
       "      <td>0.424107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.916900</td>\n",
       "      <td>1.884889</td>\n",
       "      <td>0.425912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.905900</td>\n",
       "      <td>1.876299</td>\n",
       "      <td>0.426876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.904100</td>\n",
       "      <td>1.868573</td>\n",
       "      <td>0.429704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.899100</td>\n",
       "      <td>1.863531</td>\n",
       "      <td>0.432212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.893400</td>\n",
       "      <td>1.860080</td>\n",
       "      <td>0.429472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.886200</td>\n",
       "      <td>1.854375</td>\n",
       "      <td>0.431682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.873200</td>\n",
       "      <td>1.850379</td>\n",
       "      <td>0.432384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.873500</td>\n",
       "      <td>1.847764</td>\n",
       "      <td>0.434801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.863400</td>\n",
       "      <td>1.832408</td>\n",
       "      <td>0.440942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.864500</td>\n",
       "      <td>1.826275</td>\n",
       "      <td>0.442759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.852300</td>\n",
       "      <td>1.822657</td>\n",
       "      <td>0.442481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.848700</td>\n",
       "      <td>1.820803</td>\n",
       "      <td>0.441901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.847700</td>\n",
       "      <td>1.810969</td>\n",
       "      <td>0.443483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.842700</td>\n",
       "      <td>1.820225</td>\n",
       "      <td>0.446279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.832100</td>\n",
       "      <td>1.801710</td>\n",
       "      <td>0.448273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.827300</td>\n",
       "      <td>1.786543</td>\n",
       "      <td>0.451861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.824300</td>\n",
       "      <td>1.803456</td>\n",
       "      <td>0.448621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.815300</td>\n",
       "      <td>1.785279</td>\n",
       "      <td>0.455409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.814800</td>\n",
       "      <td>1.794945</td>\n",
       "      <td>0.451598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.813000</td>\n",
       "      <td>1.796978</td>\n",
       "      <td>0.449839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.801800</td>\n",
       "      <td>1.788910</td>\n",
       "      <td>0.456361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.796300</td>\n",
       "      <td>1.783287</td>\n",
       "      <td>0.456788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.796400</td>\n",
       "      <td>1.773086</td>\n",
       "      <td>0.457606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.794100</td>\n",
       "      <td>1.779826</td>\n",
       "      <td>0.457053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.787100</td>\n",
       "      <td>1.763408</td>\n",
       "      <td>0.459769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.776400</td>\n",
       "      <td>1.759175</td>\n",
       "      <td>0.461203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.775400</td>\n",
       "      <td>1.759456</td>\n",
       "      <td>0.462498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.772800</td>\n",
       "      <td>1.753246</td>\n",
       "      <td>0.467362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.769000</td>\n",
       "      <td>1.751278</td>\n",
       "      <td>0.463126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.762500</td>\n",
       "      <td>1.753272</td>\n",
       "      <td>0.464024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.758900</td>\n",
       "      <td>1.731906</td>\n",
       "      <td>0.469635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.751700</td>\n",
       "      <td>1.728554</td>\n",
       "      <td>0.471202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.748600</td>\n",
       "      <td>1.738645</td>\n",
       "      <td>0.470181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.747900</td>\n",
       "      <td>1.722295</td>\n",
       "      <td>0.474845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.742600</td>\n",
       "      <td>1.718368</td>\n",
       "      <td>0.475789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.742900</td>\n",
       "      <td>1.713112</td>\n",
       "      <td>0.479338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.736700</td>\n",
       "      <td>1.727486</td>\n",
       "      <td>0.473508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.734900</td>\n",
       "      <td>1.729319</td>\n",
       "      <td>0.472370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.730500</td>\n",
       "      <td>1.690489</td>\n",
       "      <td>0.482706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.724100</td>\n",
       "      <td>1.715696</td>\n",
       "      <td>0.478667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.721700</td>\n",
       "      <td>1.709701</td>\n",
       "      <td>0.478831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.718300</td>\n",
       "      <td>1.696120</td>\n",
       "      <td>0.482508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.711000</td>\n",
       "      <td>1.685121</td>\n",
       "      <td>0.486620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.709800</td>\n",
       "      <td>1.706136</td>\n",
       "      <td>0.479979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.708100</td>\n",
       "      <td>1.700482</td>\n",
       "      <td>0.483883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.706700</td>\n",
       "      <td>1.684455</td>\n",
       "      <td>0.484613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.700900</td>\n",
       "      <td>1.694824</td>\n",
       "      <td>0.482734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.700700</td>\n",
       "      <td>1.690152</td>\n",
       "      <td>0.482727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.697100</td>\n",
       "      <td>1.669908</td>\n",
       "      <td>0.487385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.691700</td>\n",
       "      <td>1.680705</td>\n",
       "      <td>0.486559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.682800</td>\n",
       "      <td>1.684574</td>\n",
       "      <td>0.485530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.687800</td>\n",
       "      <td>1.680966</td>\n",
       "      <td>0.488427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.681400</td>\n",
       "      <td>1.676568</td>\n",
       "      <td>0.490915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>1.676676</td>\n",
       "      <td>0.487925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.677500</td>\n",
       "      <td>1.675456</td>\n",
       "      <td>0.488264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.671600</td>\n",
       "      <td>1.673434</td>\n",
       "      <td>0.486215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.671900</td>\n",
       "      <td>1.671564</td>\n",
       "      <td>0.492320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.672000</td>\n",
       "      <td>1.663995</td>\n",
       "      <td>0.490285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.666700</td>\n",
       "      <td>1.659376</td>\n",
       "      <td>0.494993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.659000</td>\n",
       "      <td>1.656507</td>\n",
       "      <td>0.495560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.655100</td>\n",
       "      <td>1.641394</td>\n",
       "      <td>0.498922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.658600</td>\n",
       "      <td>1.647255</td>\n",
       "      <td>0.497571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.654300</td>\n",
       "      <td>1.641998</td>\n",
       "      <td>0.502273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.651100</td>\n",
       "      <td>1.644988</td>\n",
       "      <td>0.498081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.649000</td>\n",
       "      <td>1.646994</td>\n",
       "      <td>0.495822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.645700</td>\n",
       "      <td>1.640675</td>\n",
       "      <td>0.499278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.644500</td>\n",
       "      <td>1.650418</td>\n",
       "      <td>0.497616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.639900</td>\n",
       "      <td>1.629389</td>\n",
       "      <td>0.500581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.635400</td>\n",
       "      <td>1.639199</td>\n",
       "      <td>0.500328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.636200</td>\n",
       "      <td>1.640286</td>\n",
       "      <td>0.499558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.632200</td>\n",
       "      <td>1.640978</td>\n",
       "      <td>0.498384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.630500</td>\n",
       "      <td>1.633139</td>\n",
       "      <td>0.499787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.630900</td>\n",
       "      <td>1.632346</td>\n",
       "      <td>0.502206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.620900</td>\n",
       "      <td>1.634589</td>\n",
       "      <td>0.498298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.625400</td>\n",
       "      <td>1.620269</td>\n",
       "      <td>0.505406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.624700</td>\n",
       "      <td>1.616212</td>\n",
       "      <td>0.506646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.618100</td>\n",
       "      <td>1.609998</td>\n",
       "      <td>0.509482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.618200</td>\n",
       "      <td>1.637273</td>\n",
       "      <td>0.498773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.619300</td>\n",
       "      <td>1.615319</td>\n",
       "      <td>0.506943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.618600</td>\n",
       "      <td>1.626489</td>\n",
       "      <td>0.504631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.608300</td>\n",
       "      <td>1.611387</td>\n",
       "      <td>0.506543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.605900</td>\n",
       "      <td>1.632470</td>\n",
       "      <td>0.500886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.603800</td>\n",
       "      <td>1.614255</td>\n",
       "      <td>0.506933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.605200</td>\n",
       "      <td>1.614899</td>\n",
       "      <td>0.507872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.599500</td>\n",
       "      <td>1.611360</td>\n",
       "      <td>0.507399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.600700</td>\n",
       "      <td>1.601876</td>\n",
       "      <td>0.510847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.599100</td>\n",
       "      <td>1.609828</td>\n",
       "      <td>0.505854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.592900</td>\n",
       "      <td>1.608782</td>\n",
       "      <td>0.508978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.595400</td>\n",
       "      <td>1.627075</td>\n",
       "      <td>0.505189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.594100</td>\n",
       "      <td>1.599335</td>\n",
       "      <td>0.510795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.589300</td>\n",
       "      <td>1.592092</td>\n",
       "      <td>0.512480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.587900</td>\n",
       "      <td>1.603536</td>\n",
       "      <td>0.511483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.585200</td>\n",
       "      <td>1.597927</td>\n",
       "      <td>0.513741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.589000</td>\n",
       "      <td>1.598731</td>\n",
       "      <td>0.511565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.581600</td>\n",
       "      <td>1.592435</td>\n",
       "      <td>0.516256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.583600</td>\n",
       "      <td>1.584661</td>\n",
       "      <td>0.515455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.577900</td>\n",
       "      <td>1.606051</td>\n",
       "      <td>0.511445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.578500</td>\n",
       "      <td>1.584149</td>\n",
       "      <td>0.513283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.573800</td>\n",
       "      <td>1.587185</td>\n",
       "      <td>0.516814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.578200</td>\n",
       "      <td>1.590629</td>\n",
       "      <td>0.514839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39msave_folder,\n\u001b[1;32m      5\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     eval_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;66;03m# Transfer GPU tensors during validation to RAM\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     preprocess_logits_for_metrics\u001b[38;5;241m=\u001b[39m preprocess_logits_for_metrics\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2717\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.05)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=200,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=os.path.join(save_folder, \"logs\"),            # directory for storing logs\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"mlewand/PROTBERT_LM-L8_hid512\",\n",
    "    eval_accumulation_steps = 8 # Transfer GPU tensors during validation to RAM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = data_collator,\n",
    "    preprocess_logits_for_metrics= preprocess_logits_for_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "    \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=200,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=os.path.join(save_folder, \"logs\"),            # directory for storing logs\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"mlewand/PROTBERT_LM-L8_hid512\",\n",
    "    eval_accumulation_steps = 8 # Transfer GPU tensors during validation to RAM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = data_collator,\n",
    "    preprocess_logits_for_metrics= preprocess_logits_for_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEyPO9FO2AnB"
   },
   "outputs": [],
   "source": [
    "from  transformers import pipeline\n",
    "\n",
    "model=model.to(\"cpu\")\n",
    "p = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqsxBCCj29qE"
   },
   "outputs": [],
   "source": [
    "seq = train_sentences.iloc[0]\n",
    "seq = seq[:-1]\n",
    "seq += '<mask>'\n",
    "\n",
    "p(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FfitG5KqchU"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo\n",
    "from torchinfo import summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GdYSkcBy6Bx"
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your logic for gettin thedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9huIbf-zF3T"
   },
   "outputs": [],
   "source": [
    "def getModel1():\n",
    "    return keras.Sequential(\n",
    "    [\n",
    "        // model .....\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = getModel1()\n",
    "print(model.summary())\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=20, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "052128a69fe746a1bd79d25e4d472063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_195e23c2e6074fe1a4da96bc628db141",
      "placeholder": "​",
      "style": "IPY_MODEL_3f592bd421f0466f84b2c5d37cd8b804",
      "value": "Map: 100%"
     }
    },
    "1307a4f5fe064f20a0bef031d7586a46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cdace6109dd494fbe002c3a82e39c64",
      "max": 439493,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e279b240dbe4150bb742dc8dec92401",
      "value": 439493
     }
    },
    "195e23c2e6074fe1a4da96bc628db141": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e279b240dbe4150bb742dc8dec92401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "22d4adb90ca74c66b976dd648ea5f1b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32c4135d36874d72afededd170c6f050": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f592bd421f0466f84b2c5d37cd8b804": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f5e468ca9424cb29b7551fd7d54c3bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63258ae8c5cd441ea2091e8e5f4eb797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32c4135d36874d72afededd170c6f050",
      "placeholder": "​",
      "style": "IPY_MODEL_95bbd4a7fedb46088d0012e61b688ba2",
      "value": "Map:  99%"
     }
    },
    "726e137c6dd14657af0a8e4155ad57b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63258ae8c5cd441ea2091e8e5f4eb797",
       "IPY_MODEL_84a64e0330654408a4eb6a2238437c11",
       "IPY_MODEL_9bbffee00a8242098ea61e6e863bd240"
      ],
      "layout": "IPY_MODEL_c680f4b4275444e3847082461f85225a"
     }
    },
    "7cdace6109dd494fbe002c3a82e39c64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80c9725169524de5b1c33998e8961178": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84a64e0330654408a4eb6a2238437c11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22d4adb90ca74c66b976dd648ea5f1b3",
      "max": 54378,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d33e1cb843694f3681061cd0770258fb",
      "value": 54378
     }
    },
    "95bbd4a7fedb46088d0012e61b688ba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bbffee00a8242098ea61e6e863bd240": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d686d428facd48da82133c0dc58c1327",
      "placeholder": "​",
      "style": "IPY_MODEL_5f5e468ca9424cb29b7551fd7d54c3bd",
      "value": " 54000/54378 [00:03&lt;00:00, 14622.26 examples/s]"
     }
    },
    "c365487441c04a12aabec22d86bd4561": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_052128a69fe746a1bd79d25e4d472063",
       "IPY_MODEL_1307a4f5fe064f20a0bef031d7586a46",
       "IPY_MODEL_ed9146079caa4e3baf5342d051ff2fcf"
      ],
      "layout": "IPY_MODEL_c448ff0f58654c04a883c0f9296c1301"
     }
    },
    "c448ff0f58654c04a883c0f9296c1301": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "c680f4b4275444e3847082461f85225a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "d33e1cb843694f3681061cd0770258fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d686d428facd48da82133c0dc58c1327": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea402b4672a0417b81b2d7c4c1a49ebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed9146079caa4e3baf5342d051ff2fcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80c9725169524de5b1c33998e8961178",
      "placeholder": "​",
      "style": "IPY_MODEL_ea402b4672a0417b81b2d7c4c1a49ebf",
      "value": " 438000/439493 [00:30&lt;00:00, 15568.91 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
